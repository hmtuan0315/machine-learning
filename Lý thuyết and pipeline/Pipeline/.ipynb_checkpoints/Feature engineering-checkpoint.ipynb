{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749bf1c9",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b302a",
   "metadata": {},
   "source": [
    "## 1. Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91fde7a",
   "metadata": {},
   "source": [
    "### a. Drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f006c",
   "metadata": {},
   "source": [
    "**There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing values with higher than this threshold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02908b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1acf44",
   "metadata": {},
   "source": [
    "### b.Numerical Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a45cb",
   "metadata": {},
   "source": [
    "**I think the best imputation way is to use the medians of the columns. As the averages of the columns are sensitive to the outlier values, while medians are more solid in this respect.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7093fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d707f",
   "metadata": {},
   "source": [
    "### c. Categorical Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdd732",
   "metadata": {},
   "source": [
    "**Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c950e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts()\n",
    ".idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a15be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2529dc12",
   "metadata": {},
   "source": [
    "## 2. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf925fe2",
   "metadata": {},
   "source": [
    "### a.Outlier Detection with Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddca21",
   "metadata": {},
   "source": [
    "### b.Outlier Detection with Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fe646",
   "metadata": {},
   "source": [
    "### c.An Outlier Dilemma: Drop or Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c02d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143be4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22bda0d",
   "metadata": {},
   "source": [
    "## 3.Log Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4a451",
   "metadata": {},
   "source": [
    "**The benefits of log transform:**\n",
    "\n",
    "**- It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.**\n",
    "\n",
    "**- In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not **equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.**\n",
    "\n",
    "**- It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c27d4",
   "metadata": {},
   "source": [
    "**A critical note: The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfcf823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
    "   value  log(x+1)  log(x-min(x)+1)\n",
    "0      2   1.09861          3.25810\n",
    "1     45   3.82864          4.23411\n",
    "2    -23       nan          0.00000\n",
    "3     85   4.45435          4.69135\n",
    "4     28   3.36730          3.95124\n",
    "5      2   1.09861          3.25810\n",
    "6     35   3.58352          4.07754\n",
    "7    -12       nan          2.48491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e828744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebb31af",
   "metadata": {},
   "source": [
    "## 4. One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "data = data.join(encoded_columns).drop('column', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0476f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b5484bc",
   "metadata": {},
   "source": [
    "## 5.Grouping Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0379c6",
   "metadata": {},
   "source": [
    "**Datasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. In such a case, we group the data by the instances and then every instance is represented by only one row.**\n",
    "\n",
    "**The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f9c15",
   "metadata": {},
   "source": [
    "### a.Categorical Column Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa15b6",
   "metadata": {},
   "source": [
    "**- The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.**\n",
    "\n",
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb37cd8",
   "metadata": {},
   "source": [
    "**Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative.**\n",
    "\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)\n",
    "\n",
    "<img src=\"../Lý thuyết/ml_lý_thuyết/Screenshot 2021-07-05 113308.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496c0e2",
   "metadata": {},
   "source": [
    "**Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. You can check the next section for the explanation of numerical column grouping.\n",
    "Numerical Column Grouping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4453c",
   "metadata": {},
   "source": [
    "### b.Numerical Column Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43be3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (conda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
