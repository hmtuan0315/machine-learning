{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c210258",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a05769",
   "metadata": {},
   "source": [
    "**How to select features and what are Benefits of performing feature selection before modeling your data?**\n",
    "\n",
    "**- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.**\n",
    "\n",
    "**- Improves Accuracy: Less misleading data means modeling accuracy improves**\n",
    "\n",
    "**- Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70621b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:55:50.314785Z",
     "start_time": "2021-07-05T07:55:50.304760Z"
    }
   },
   "source": [
    "<img src=\"../Lý thuyết/ml_lý_thuyết/Screenshot 2021-07-05 125732.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccf4e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:24:58.824110Z",
     "start_time": "2021-07-05T08:24:58.804164Z"
    }
   },
   "source": [
    "<img src=\"../Lý thuyết/ml_lý_thuyết/Screenshot 2021-07-05 152027.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af70c4",
   "metadata": {},
   "source": [
    "## 1.Filter methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81639e5a",
   "metadata": {},
   "source": [
    "**Filter methods pick up the intrinsic properties of the features measured via univariate statistics instead of cross-validation performance. These methods are faster and less computationally expensive than wrapper methods. When dealing with high-dimensional data, it is computationally cheaper to use filter methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a601a59",
   "metadata": {},
   "source": [
    "### a.Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e761d9",
   "metadata": {},
   "source": [
    "**Information gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc124ce",
   "metadata": {},
   "source": [
    "<img src=\"../Lý thuyết/ml_lý_thuyết/Screenshot 2021-07-05 151518.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa04588",
   "metadata": {},
   "source": [
    "### b. Chi-squared test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2fea8",
   "metadata": {},
   "source": [
    "**The Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. In order to correctly apply the chi-squared in order to test the relation between various features in the dataset and the target variable, the following conditions have to be met: the variables have to be categorical, sampled independently and values should have an expected frequency greater than 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efab41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:31:28.738668Z",
     "start_time": "2021-07-05T08:31:28.726641Z"
    }
   },
   "source": [
    "<img src=\"../Lý thuyết/ml_lý_thuyết/Screenshot 2021-07-05 153111.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f5b6f",
   "metadata": {},
   "source": [
    "### c.Pearson's Correlation (linear correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdacb0",
   "metadata": {},
   "source": [
    "**If two variables are correlated, we can predict one from the other. Therefore, if two features are correlated, the model only really needs one of them, as the second one does not add additional information. We will use the Pearson Correlation here.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb917c",
   "metadata": {},
   "source": [
    "<img src=\"../Lý thuyết/ml_lý_thuyết/Screenshot 2021-07-05 153443.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a505221",
   "metadata": {},
   "source": [
    "**We need to set an absolute value, say 0.5 as the threshold for selecting the variables. If we find that the predictor variables are correlated among themselves, we can drop the variable which has a lower correlation coefficient value with the target variable. We can also compute multiple correlation coefficients to check whether more than two variables are correlated to each other. This phenomenon is known as multicollinearity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe8623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (conda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
